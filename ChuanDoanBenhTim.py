import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb

pd.set_option('future.no_silent_downcasting', True)

# B1: ƒê·ªçc d·ªØ li·ªáu
file_path = r'C:\Users\wbu86\Downloads\archive (1)\heart_disease_uci.csv'
data = pd.read_csv(file_path)

# B2: Hi·ªÉn th·ªã th√¥ng tin d·ªØ li·ªáu
print("Th√¥ng tin d·ªØ li·ªáu:")
print(data.info())
print("\nD·ªØ li·ªáu m·∫´u:")
print(data.head())

# X·ª≠ l√Ω gi√° tr·ªã thi·∫øu
for col in data.columns:
    if data[col].dtype == 'object':
        data.fillna({col: data[col].mode()[0]}, inplace=True)
    else:
        data.fillna({col: data[col].median()}, inplace=True)

data = data.infer_objects(copy=False)

# Ki·ªÉm tra l·∫°i gi√° tr·ªã thi·∫øu
print("\nGi√° tr·ªã thi·∫øu sau khi x·ª≠ l√Ω:")
print(data.isnull().sum())

# B3: M√£ h√≥a c·ªôt chu·ªói th√†nh s·ªë
data = pd.get_dummies(data)

# Ki·ªÉm tra c·ªôt m·ª•c ti√™u (target)
target_column = 'target' if 'target' in data.columns else data.columns[-1]

# V·∫Ω heatmap ch·ªâ v·ªõi c·ªôt s·ªë
plt.figure(figsize=(12, 8))
sns.heatmap(data.corr(), annot=True, fmt=".2f", cmap='viridis', linewidths=0.5)
plt.title('Correlation Matrix of Features')
plt.show()

# Bi·ªÉu ƒë·ªì ph√¢n b·ªë d·ªØ li·ªáu theo nh√£n (target)
plt.figure(figsize=(8, 6))
sns.countplot(x=target_column, hue=target_column, data=data, palette=['#9CC9E2', '#E57373'], legend=False)
plt.title('Ph√¢n b·ªë nh√£n: 0 (Kh√¥ng m·∫Øc b·ªánh) v√† 1 (M·∫Øc b·ªánh tim)')
plt.xlabel('0: No Disease, 1: Heart Disease')
plt.ylabel('Count')
plt.show()

# T√°ch ƒë·∫∑c tr∆∞ng (features) v√† nh√£n (target)
features = data.drop(columns=target_column)
target = data[target_column]

# Chu·∫©n h√≥a d·ªØ li·ªáu
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# B4: Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra
X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)

# B5: Kh·ªüi t·∫°o m√¥ h√¨nh Keras
model = Sequential([
    Input(shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),
    Dense(16, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

# Bi√™n d·ªãch m√¥ h√¨nh
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Hu·∫•n luy·ªán m√¥ h√¨nh
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)

# B6: ƒê√°nh gi√° m√¥ h√¨nh Keras
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')

# V·∫Ω bi·ªÉu ƒë·ªì Accuracy v√† Loss
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

# B7: Hu·∫•n luy·ªán v√† ƒë√°nh gi√° c√°c m√¥ h√¨nh ML

def evaluate_model(model, model_name):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f'\nüéØ {model_name} Accuracy: {acc:.4f}')
    print(classification_report(y_test, y_pred))

    plt.figure(figsize=(6, 4))
    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
    plt.title(f'{model_name} - Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

# 1. Logistic Regression
log_model = LogisticRegression()
evaluate_model(log_model, "Logistic Regression")

# 2. Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
evaluate_model(rf_model, "Random Forest")

# 3. XGBoost
xgb_model = xgb.XGBClassifier(eval_metric='logloss')
evaluate_model(xgb_model, "XGBoost")
